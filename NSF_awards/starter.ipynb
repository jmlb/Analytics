{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTHON WRAPPER FOR NSF AWARD SEARCH API\n",
    "\n",
    "https://www.research.gov/common/webapi/awardapisearch-v1.htm\n",
    "\n",
    "As per NSF notification, there could be downtime, service disruption which are typically on weekends starting at 10PM Friday through 12PM Sunday. so, if you get an error like:\n",
    "```python\n",
    "HTTPError: HTTP Error 503: Service Temporarily Unavailable\n",
    "```\n",
    "\n",
    "it is likely because the server is going thru some maintenance.\n",
    "\n",
    "Record Offset\tNo\toffset\tEnter the record offset (always starts with 1). This is used in conjunction with results per page to fetch large data sets in chunks. For example, if a search produces 82 results and the result per page is set to 25, this will generate 4 set of pages. 3 pages will have 25 results and the last page will have 7 results. Record offset value will be\n",
    "Page 1: offset=1\n",
    "Page 2: offset=26\n",
    "Page 3: offset=51\n",
    "Page 4: offset=76\n",
    "\n",
    "\n",
    "check the website for more info\n",
    "\n",
    "save on a database SQLite for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import sqlite3\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REQUEST NSF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of existing award: \n",
      " {'response': {'award': [{'piLastName': 'Boyd', 'awardeeCity': 'Newark', 'title': 'Regulatory Pathways of the Osmotic Stress Response in Bacteria', 'id': '1656688', 'awardeeName': 'University of Delaware', 'date': '03/12/2017', 'awardeeStateCode': 'DE', 'fundsObligatedAmt': '237824', 'publicAccessMandate': '1', 'agency': 'NSF', 'piFirstName': 'Fidelma'}]}} \n",
      " \n",
      "\n",
      "Example of Non-existing award: \n",
      " {'response': {'award': []}} \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://api.nsf.gov/services/v1/awards.json?'\n",
    "params = urllib.parse.urlencode({'id': 1656688})\n",
    "response = urllib.request.urlopen('{}{}'.format(url, params) )\n",
    "data_r = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "print('Example of existing award: \\n {} \\n \\n'.format(data_r))\n",
    "\n",
    "params = urllib.parse.urlencode({'id': 1})\n",
    "response = urllib.request.urlopen('{}{}'.format(url, params) )\n",
    "data_r = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "print('Example of Non-existing award: \\n {} \\n \\n'.format(data_r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'response': {'serviceNotification': [{'notificationType': 'ERROR', 'notificationCode': 'AwardAPI-002', 'notificationMessage': 'Invalid parameter(s) sent in the request. Invalid Parameter(s) {pageStart}'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save data in SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ead8b0838ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0moverwrite_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mScraper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAwardScraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mScraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect2_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mn_awards_per_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6ead8b0838ba>\u001b[0m in \u001b[0;36mconnect2_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect2_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msql_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'*.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msql_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moverwrite_db\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from urllib.parse import quote\n",
    "import re\n",
    " # delays for 5 seconds\n",
    "\n",
    "\n",
    "    \n",
    "class AwardScraper():\n",
    "    \n",
    "    def __init__(self, url, folder, dbname, table_name, overwrite_db):\n",
    "        self.url = url\n",
    "        self.table_name = table_name\n",
    "        self.folder = folder\n",
    "        self.dbname = dbname\n",
    "        self.overwrite_db = overwrite_db\n",
    "        self.failed_id = []\n",
    "        self.n_fail = 0\n",
    "        self.n_awards_in_page = 0\n",
    "    \n",
    "    \n",
    "    def clean_str(self, s):\n",
    "        clean = re.sub('[^/A-Za-z0-9-%]+', ' ', s)\n",
    "        return clean\n",
    "        \n",
    "    def connect2_db(self):\n",
    "        sql_files = glob.glob(self.folder+'*.db')\n",
    "        if (self.dbname in sql_files) or (overwrite_db == False):\n",
    "            \n",
    "            self.conn = sqlite3.connect(self.folder+dbname)\n",
    "            \n",
    "            self.c = self.conn.cursor()\n",
    "        else:\n",
    "            #open connection to local file\n",
    "            self.conn = sqlite3.connect(self.folder+dbname)\n",
    "            self.c = self.conn.cursor()\n",
    "            # Create table\n",
    "            try:\n",
    "                self.c.execute(\"CREATE TABLE research_awards (id, agency, awardeeCity, awardeeName, awardeeStateCode, fundsObligatedAmount, piFirstname, piLastName, publicAccessMandate, date, title, topic))\".format(table_name))\n",
    "                print('Created Table: {} in database {}'.format(table_name, self.folder+dbname) )\n",
    "                # Save (commit) the changes\n",
    "                self.conn.commit()\n",
    "            except:\n",
    "                print('Unable to create table')\n",
    "\n",
    "            \n",
    "    \n",
    "    def fetch_info(self, request):\n",
    "        '''\n",
    "        request is a dict {'dateStart': value}\n",
    "        \n",
    "        '''\n",
    "        params = urllib.parse.urlencode(request)\n",
    "        response = urllib.request.urlopen('{}{}'.format(self.url, params), timeout=180)\n",
    "        response_json = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "        response_json = response_json['response']\n",
    "        if 'award' in response_json:\n",
    "            awards = response_json['award']\n",
    "            print('Number of awards: {}'.format(len(awards)) )\n",
    "            return awards\n",
    "        else:\n",
    "            print(response_json['serviceNotification'])\n",
    "            return [] \n",
    "    \n",
    "    \n",
    "    def clean_award_data(self, info):\n",
    "        categories = ['piLastName', 'awardeeCity', 'title', 'id', 'awardeeName', 'date', 'awardeeStateCode',\n",
    "                      'fundsObligatedAmt', 'publicAccessMandate', 'agency', 'piFirstName']\n",
    "        for category in categories:\n",
    "            if category not in info:\n",
    "                info[category] = ''\n",
    "            info[category] = self.clean_str(info[category])\n",
    "        \n",
    "        return info\n",
    "                \n",
    "        \n",
    "    \n",
    "    def save_data2db(self, item):\n",
    "        sql_command = \"SELECT research_awards.id FROM research_awards WHERE research_awards.id = '{}';\".format(item['id'])\n",
    "        try:\n",
    "            self.c.execute(sql_command)\n",
    "            result = self.c.fetchall()\n",
    "            if len(result) != 0:\n",
    "                item_in_table = True\n",
    "            else:\n",
    "                item_in_table = False\n",
    "            err_select = False\n",
    "        except:\n",
    "            # Insert a row of data\n",
    "            print('Unable to execute SQL query: SELECT')\n",
    "            err_select = True   \n",
    "        if item_in_table == False:\n",
    "            item = self.clean_award_data(item)\n",
    "            sql_command = \"INSERT INTO research_awards VALUES ('{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}');\".format(item['id'], \n",
    "            item['agency'], \n",
    "            item['awardeeCity'], item['awardeeName'], item['awardeeStateCode'], \n",
    "            item['fundsObligatedAmt'], item['piFirstName'], item['piLastName'], \n",
    "            item['publicAccessMandate'], item['date'], item['title'], \"\")\n",
    "            try:\n",
    "                self.c.execute(sql_command)\n",
    "                self.conn.commit()\n",
    "                return True\n",
    "            except:\n",
    "                print('Unable to save data on database')\n",
    "                return False\n",
    "\n",
    "            \n",
    "    def scan_page(self, page):\n",
    "        for idx in range(len(page)):\n",
    "            flag = self.save_data2db(page[idx])\n",
    "            if flag==False:\n",
    "                self.n_fail = self.n_fail + 1\n",
    "        self.n_awards_in_page = len(page)\n",
    "\n",
    "                \n",
    "                \n",
    "sleep_time = 0.01\n",
    "url = 'http://api.nsf.gov/services/v1/awards.json?'\n",
    "date ='03/16/1980'\n",
    "flder = ''\n",
    "table_name = \"research_awards\"\n",
    "dbname = 'research_grants.db'\n",
    "overwrite_db = True\n",
    "Scraper = AwardScraper(url, '', dbname, table_name, overwrite_db)\n",
    "Scraper.connect2_db()\n",
    "\n",
    "n_awards_per_page = 25\n",
    "n_awards = n_awards_per_page\n",
    "offset = 77178\n",
    "\n",
    "while n_awards > 0:\n",
    "    print('Page number: {}'.format(offset) ) \n",
    "    request = {'dateStart': date, 'offset': offset}\n",
    "    response = Scraper.fetch_info(request)\n",
    "    if len(response) != 0:\n",
    "        Scraper.scan_page(response)\n",
    "        n_awards = Scraper.n_awards_in_page\n",
    "        offset += n_awards_per_page\n",
    "    else:\n",
    "        print('Page empty')\n",
    "        n_awards = -1   \n",
    "\n",
    "#url = 'http://api.nsf.gov/services/v1/awards.json?'\n",
    "#params = urllib.parse.urlencode({'id': 1656688})\n",
    "#response = urllib.request.urlopen('{}{}'.format(url, params) )\n",
    "#data_r = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "#print('Example of existing award: \\n {} \\n \\n'.format(data_r))\n",
    "\n",
    "\n",
    "#if flag_save == True:\n",
    "#        n_award += 1\n",
    "#    #time.sleep(sleep_time)\n",
    "#    duration = round(time.time() - start, 0)\n",
    "#    if duration%100  == 0:\n",
    "#        print('Which one: ', idx)\n",
    "    \n",
    "#print('Total Number of award saved: {}'.format(n_award))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
